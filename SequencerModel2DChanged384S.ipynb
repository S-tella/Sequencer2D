{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiLSTMVertical:\n",
    "This is a module that implements a Bidirectional Long Short-Term Memory (BiLSTM) layer with a vertical direction of computation.\n",
    "It takes two arguments: input_size and hidden_size, which determine the input and hidden state dimensions of the LSTM.\n",
    "Inside the __init__ method, it initializes an nn.LSTM module with the specified input and hidden sizes. It's set to be bidirectional (bidirectional=True) and accepts input data in batches (batch_first=True).\n",
    "In the forward method, it applies the BiLSTM to the input tensor x and returns the output.\n",
    "BiLSTMHorizontal:\n",
    "This is another module similar to BiLSTMVertical, but it implements a BiLSTM layer with a horizontal direction of computation.\n",
    "It has the same constructor and forward method structure as BiLSTMVertical.\n",
    "FullyConnectedLayer:\n",
    "This module represents a fully connected (linear) layer in a neural network.\n",
    "It takes two arguments: input_size and output_size, determining the input and output dimensions of the linear layer.\n",
    "In the __init__ method, it initializes an nn.Linear module with the specified input and output sizes.\n",
    "In the forward method, it applies the linear transformation to the input tensor x and returns the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTMVertical(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BiLSTMVertical, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)\n",
    "        return output\n",
    "\n",
    "class BiLSTMHorizontal(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BiLSTMHorizontal, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)\n",
    "        return output\n",
    "\n",
    "class FullyConnectedLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FullyConnectedLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequencer2D\n",
    "Is a sequencer in which the input data (images) are splitted into vertical adn horiziontal component, then vertical data will be passed as input to BilstmVertical ,while horizontal_data will be passed to BiLSTMHorizontal, to do so the input data are first processed in such a way we'll have a tensor with vertical data of each image and another tensor with horizontal data of each image. Oncee we get the outputs from both BiLSTMvertical and BiLSTMHorizontal, we 'll concatenate them and pass as input the result to Fully Connected Layer . At the end of the sequencer the results will be a list of tensor ( each tensor has inside data belongs to each image processed above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Sequencer2D(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, mlp_input_size, mlp_output_size):\n",
    "        super(Sequencer2D, self).__init__()\n",
    "\n",
    "        # Create instances of the defined models\n",
    "        vertical_bilstm = BiLSTMVertical(input_size, hidden_size)\n",
    "        horizontal_bilstm = BiLSTMHorizontal(input_size, hidden_size)\n",
    "        fully_connected = FullyConnectedLayer( mlp_input_size,mlp_output_size)  \n",
    "\n",
    "        # Move models to GPU if available\n",
    "        #vertical_bilstm.to(device)\n",
    "        #horizontal_bilstm.to(device)\n",
    "        #fully_connected.to(device)\n",
    "\n",
    "        self.vertical_bilstm = vertical_bilstm\n",
    "        self.horizontal_bilstm = horizontal_bilstm\n",
    "        self.fully_connected = fully_connected\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, list):\n",
    "            x = torch.stack(x)\n",
    "        if len(x.shape)==4:\n",
    "            batch_size,channel, height, width = x.size()\n",
    "            x=x.squeeze(1)\n",
    "        elif len(x.shape) == 3 :\n",
    "            batch_size, height, width = x.size()\n",
    "        output=[]\n",
    "        for index in range(batch_size):\n",
    "            x_vertical = x[index][:, :height//2]  #split the image vertically\n",
    "            x_horizontal = x[index][ :,width//2:]   #split image horizontally\n",
    "\n",
    "            vertical_data = x_vertical.permute( 1,0) #permutation to fit bilstm\n",
    "            horizontal_data = x_horizontal.permute( 1,0)#same here\n",
    "            \n",
    "            \n",
    "            \n",
    "            H_ver = self.vertical_bilstm(vertical_data)\n",
    "            H_hor = self.horizontal_bilstm(horizontal_data)\n",
    "            \n",
    "            \n",
    "            H_ver = torch.tensor(H_ver, dtype=torch.float32).permute(1,0) #permute to fit torch.cat\n",
    "            H_hor = torch.tensor(H_hor, dtype=torch.float32).permute(1,0)\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            H_concat = torch.cat((H_ver, H_hor), dim=1)#concatenaiton of the horizontal and vertical bilstm outputs\n",
    "            H_concat = H_concat.unsqueeze(0) #set dimension to fit fully conncted layer\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            #fully connected layer\n",
    "            \n",
    "            \n",
    "            output_pieces = self.fully_connected(H_concat)\n",
    "            \n",
    "            output_pieces=torch.tensor(output_pieces, dtype=torch.float32)\n",
    "            output.append(output_pieces) #add output to lists of each image data processed here\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch Embedding : this costum function given an image , a patch size, will splitted the image in image_size/patch_size pieces.\n",
    "\n",
    "image_size: The size of the input image. It represents both the height and width of the image.\n",
    "patch_size: The size of each non-overlapping patch that the image is divided into.\n",
    "in_channels: The number of input channels in the image. For RGB images, this is typically 3 (for the red, green, and blue channels).\n",
    "embed_dim: The dimension of the embedded representations for each patch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        \n",
    "        self.num_patches = (image_size // patch_size) ** 2 #compute the number of patches in such a way they won't overlap\n",
    "\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size) #convolutional layer takes the input image, divides it into patches, and embeds each patch into a lower-dimensional representation.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x should have the shape (B, C, H, W) if needed: B, C, H, W = x.shape\n",
    "     \n",
    "        x = self.projection(x).flatten(2).transpose(1, 2)  # (B reshape the input to split it into patches\n",
    "\n",
    "        return x\n",
    "    def output_dimension(self):\n",
    "        return self.embed_dim * self.num_patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PW Linear Layer compute  pointwise linear transformations on a list of input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PWLinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(PWLinearLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, input_list):\n",
    "        output_list = []\n",
    "        for input_tensor in input_list:\n",
    "            output_tensor = self.linear(input_tensor)\n",
    "            output_list.append(output_tensor)\n",
    "        stacked_output = torch.stack(output_list, dim=0)\n",
    "        return stacked_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch merging, take the images splitted in patches (for examples by patch embedding ) and merge it together, the number of patches merged is based on the output channlee and  the  scale factor number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor):\n",
    "        super(PatchMerging, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, patch_list):\n",
    "        \n",
    "        x = torch.cat(patch_list, dim=1).permute(1,2,0) # compination of the list of patches into a single tensor + peermutation to fit convolutional layer\n",
    "        \n",
    "        \n",
    "        x = self.conv(x) # Apply to the input x, a 1x1 convolution to merge the patches together\n",
    "        \n",
    "        \n",
    "        x = nn.functional.interpolate(x, scale_factor=self.scale_factor, mode='nearest') #resize the feature map using a nearest-neighbor upsampling\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GlobalAveragePooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAveragePooling, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.mean(x, dim=(-2, -1))# perform global average pooling along spatial dimensions using the mean\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward network not used in this code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This is the Feedforward Neural Network (FFN), I've tried to add this to the model but it slow the process too much\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, activation=nn.GELU()):\n",
    "        super(FFN, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x) #aI've tried other activ.fucntion before decided to apply GELU function () in the original code they use Gelu activation function too )\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequencer2Dmodel-> Is an architecture that resambles vision transformer except for the fact that Sequencer2D model is using BiLSTM2D (splitting image in horizzontal and vertical data) instead of self attention. It started with patch embedding layeer so the images will be divided in 8x8 patches, than ,after a normalization, there is the first sequence block of 4 Sequencer , the output of this block converge in PatchMerging, where the patches are resembled together ,then there is a second sequencer block made of 3 sequencer2d, followed by pointwise linear layer,then other sequencer block with 8 sequencers2D, then other point wise linear layer and  the final sequencerBlock with 3 sequencer2D layer and at the end there are linears+ global avarage pooling layers in order to make the output dimension as expected-\n",
    "\n",
    "The parameters are the one suggested by the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sequencer2DModel(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels):\n",
    "        super(Sequencer2DModel, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "\n",
    "       \n",
    "        self.patch_embedding_1 = PatchEmbedding(16, 8, in_channels,64 ) #  patch embedding with an 8x8 kernel size for each patch\n",
    "        self.ln_1 = nn.LayerNorm(64)\n",
    "\n",
    "       \n",
    "        self.sequencer_block_1 =  nn.Sequential(\n",
    "            Sequencer2D(16, 48, 40, 96),\n",
    "            Sequencer2D(96, 96, 96, 192),\n",
    "            Sequencer2D(192, 192, 192,384),\n",
    "            Sequencer2D(384, 192, 384,384)\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.patch_merging=PatchMerging(24576,64,2)\n",
    "        \n",
    "\n",
    "        \n",
    "        self.sequencer_block_2 =  nn.Sequential(\n",
    "            Sequencer2D(384, 192, 3,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384)\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.pw_linear_1 = PWLinearLayer( 384,384)\n",
    "        \n",
    "        \n",
    "        self.sequencer_block_3 =  nn.Sequential(\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            \n",
    "            \n",
    "           \n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # Feedforward Neural Network remove comments to use \n",
    "        #self.ffn = nn.Sequential(\n",
    "        #    nn.Linear(384,384),\n",
    "        #    nn.ReLU(),\n",
    "        #    nn.Linear(1024, 512),\n",
    "        #    nn.ReLU(),\n",
    "        #    nn.Linear(512, 256),\n",
    "        #)\n",
    "        \n",
    "        self.pw_linear_2 = PWLinearLayer(384, 384)\n",
    "\n",
    "        \n",
    "        self.sequencer_block_4 =  nn.Sequential(\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384)\n",
    "        )\n",
    "        self.pw_linear_3 = PWLinearLayer(384, 384)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        self.ln_2 = nn.LayerNorm(384)\n",
    "\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 384))\n",
    "\n",
    "        \n",
    "        self.fc = nn.Linear(384, num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        \n",
    "        x = x.to(torch.float32).permute(0,3,1,2)  # convert input tensor to float32 + permutation to fit patch embedding \n",
    "        x = self.patch_embedding_1(x)\n",
    "        x = self.ln_1(x) #linearization\n",
    "\n",
    "        \n",
    "        x = self.sequencer_block_1(x)\n",
    "\n",
    "        \n",
    "        x = self.patch_merging(x) #merging patches of images processed in bilstm\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.sequencer_block_2(x)\n",
    "        x = self.pw_linear_2(x)\n",
    "        \n",
    "        x = self.sequencer_block_3(x)\n",
    "\n",
    "        \n",
    "        # here should be the place for feedforward NN\n",
    "        #x = self.ffn(x)\n",
    "\n",
    "        # soem addditional Linear Layers after FNN \n",
    "        #x = self.fc1(x)\n",
    "        #x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        x = self.pw_linear_2(x)\n",
    "\n",
    "        \n",
    "        x = self.sequencer_block_4(x)\n",
    "        x = self.pw_linear_3(x)\n",
    "        \n",
    "        \n",
    "        x = self.ln_2(x)\n",
    "        \n",
    "        x = self.global_avg_pool(x)#apply global avg pooling \n",
    "        \n",
    "        x = self.fc(x)#final linear layer that brings the output in the desired shape\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CustomCifar2, this custom function process imace into the dataset in order to take only the first two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomCIFAR2(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
    "        super(CustomCIFAR2, self).__init__()\n",
    "        self.cifar10 = datasets.CIFAR10(root, train=train, transform=transform, target_transform=target_transform, download=download)\n",
    "        \n",
    "        \n",
    "        self.keep_classes = [0, 1]  # Class indices for airplane and automobile\n",
    "\n",
    "        \n",
    "        self.data, self.targets = self.filter_classes()#will filtered all the data in order to keep only the classes specified\n",
    "\n",
    "    def filter_classes(self):\n",
    "        mask = np.isin(self.cifar10.targets, self.keep_classes)\n",
    "        data = [self.cifar10.data[i] for i, include in enumerate(mask) if include]\n",
    "        targets = [self.cifar10.targets[i] for i, include in enumerate(mask) if include]\n",
    "        return data, targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "image_size = 32\n",
    "patch_size = 8\n",
    "model = Sequencer2DModel(num_classes=2, in_channels=3)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "batch_size = 64 # obv if batch_size changes, it must be changed the parameters of sequencer Model.\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "#This part includes transformations applied to the data,so preprocessing +data augmentation.\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),   # Randomly flip the image horizontally\n",
    "    transforms.RandomRotation(15),      # Randomly rotate the image by up to 15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8, hue=0.8),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4),# Adjust brightness, contrast, saturation, and hue\n",
    "    transforms.RandomResizedCrop(16),\n",
    "    transforms.RandomResizedCrop(4),# Randomly crop and resize the image to 224x224\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.RandomAffine(degrees=4, translate=(0.4, 0.1)),# Randomly translate the image\n",
    "    transforms.ToTensor(),              # Convert the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "#remove comment \"#\" below to use all classes, instead of 2 as used in this data_loader\n",
    "#train_dataset = torchvision.datasets.CIFAR10(root='/Users/stellafazioli/Downloads/cifar-10-batches-py', train=True, transform=transform)\n",
    "custom_dataset = CustomCIFAR2(root='/Users/stellafazioli/Downloads/cifar-10-batches-py', train=True, transform=transform, download=True)\n",
    "data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)#calls the custom dataset that has charged only the first 2 classes from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #to use GPU\n",
    "model.to(device) #save model to device\n",
    "all_labels=[]\n",
    "all_accuracy=[]\n",
    "all_loss=[]\n",
    "\n",
    "# Start the training Phase\n",
    "for epoch in range(num_epochs):\n",
    "    total_correct = 0 #total number of right prediction \n",
    "    total_samples = 0 #total number of image processed\n",
    "    running_loss = 0.0 # take the count of loss during 100 batches\n",
    "\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        if i==156:  #in order to jump \n",
    "            continue\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        outputs = model(inputs)# does forward pass by feed the model\n",
    "        \n",
    "        outputs=outputs.view(64,2)# reshape outputs\n",
    "        \n",
    "        \n",
    "        predicted_probabilities = torch.sigmoid(outputs) #tensor of prediciton over outputs using sigmoid function\n",
    "        \n",
    "        loss = criterion(predicted_probabilities[:,0], labels.float()) #compute loss (Binary crossEntropy function)\n",
    "        \n",
    "        all_loss.append(loss)\n",
    "        \n",
    "        loss.backward()#does backpropagation\n",
    "        optimizer.step()# call optimizer (Adam )\n",
    "\n",
    "        \n",
    "        running_loss += loss.item() \n",
    "        predicted_probabilities = outputs.argmax(dim=1) #reshape to compute correct prediction\n",
    "        correct = (predicted_probabilities == labels).sum().item() #compute the sum of all the prediction that are equal to labels so the  sum of all the right prediction\n",
    "        total_correct += correct #total correct predition over all epochs untill now\n",
    "        total_samples += labels.size(0) #total number of images trained over all epochs\n",
    "        \n",
    "        batch_accuracy = (correct / labels.size(0)) * 100.0 #compute accuracy percentage \n",
    "        batch_accuracy = (correct / labels.size(0)) * 100.0\n",
    "        all_accuracy.append(batch_accuracy)\n",
    "        for label in labels:\n",
    "            all_labels.append(labels)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{i}/{len(data_loader)}] Loss: {loss.item():.4f} Accuracy: {batch_accuracy:.2f}%\")\n",
    "        if i % 100==0 :  # Print every 100 mini-batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{i}/{len(data_loader)}] Loss: {loss.item():.4f} Accuracy: {batch_accuracy:.2f}%\")\n",
    "            print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    epoch_accuracy = (total_correct / total_samples) * 100.0\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "model_pkl_file = \"sequencer_model384S_binary.pkl\"\n",
    "\n",
    "with open(model_pkl_file, 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-3fb609d5316e>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  H_ver = torch.tensor(H_ver, dtype=torch.float32).permute(1,0)\n",
      "<ipython-input-3-3fb609d5316e>:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  H_hor = torch.tensor(H_hor, dtype=torch.float32).permute(1,0)\n",
      "<ipython-input-3-3fb609d5316e>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output_pieces=torch.tensor(output_pieces, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 384, 2])\n",
      "torch.Size([64, 1, 1, 384])\n",
      "batch: [115/157] Test Accuracy: 29.69%\n",
      "2\n",
      "torch.Size([64, 384, 2])\n",
      "torch.Size([64, 1, 1, 384])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-a2d36308e66b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mlab_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlab_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlab_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),   \n",
    "    transforms.RandomRotation(15),     \n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    \n",
    "    transforms.RandomResizedCrop(16),\n",
    "    transforms.RandomResizedCrop(4),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.RandomAffine(degrees=4, translate=(0.4, 0.1)),\n",
    "    transforms.ToTensor(),              \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "test_dataset = CustomCIFAR2(root='/Users/stellafazioli/Downloads/cifar-10-batches-py', train=False, transform=transform, download=True)#load cifar10 test images with custom function for dataset in order to charge only 2 classes\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)#load the test data\n",
    "\n",
    "\n",
    "# remove comment and change the file name with the path of the model in case it is needed \n",
    "# model.load_state_dict(torch.load('your_model_weights.pkl'))\n",
    "\n",
    "# Set your device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model in evaluation mode (disables dropout and batch normalization)\n",
    "model.eval()\n",
    "\n",
    "# Lists to store predictions and ground truth labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_loss=[]\n",
    "all_accuracy=[]\n",
    "c=0\n",
    "# Iterate through the test dataset and make predictions\n",
    "total_correct=0\n",
    "total_samples=0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        \n",
    "        c+=1\n",
    "        print(c)\n",
    "        if c==78:\n",
    "            continue\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "       \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate predicted labels (you may need to adjust this part based on your model)\n",
    "        # Assuming the model's output is a tensor of shape (batch_size, num_classes)\n",
    "        #predicted_labels = torch.argmax(outputs, dim=1)\n",
    "\n",
    "       \n",
    "        lab_seq=torch.stack(lab_seq)\n",
    "        all_labels.extend(lab_seq.cpu().numpy())\n",
    "        \n",
    "        predicted_labels = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        predicted_labels = predicted_labels.argmax(dim=1)\n",
    "        predicted_labels= predicted_labels.argmax(dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        correct = (predicted_labels[:,0] == labels).sum().item()\n",
    "        \n",
    "        total_correct += correct\n",
    "        total_samples += lab_seq.size(0)\n",
    "        \n",
    "        accuracy = (total_correct / total_samples) * 100.0  \n",
    "        print(f\"batch: [{i}/{len(data_loader)}]\",f\"Test Accuracy: {accuracy :.2f}%\")\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "all_loss=torch.tensor(all_loss)\n",
    "# this code will plot loss and accuracy\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(all_loss.detach().numpy(), label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Loss Over Time')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(all_accuracy, label='Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Accuracy Over Time')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
