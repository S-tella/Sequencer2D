{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiLSTMVertical:\n",
    "This is a module that implements a Bidirectional Long Short-Term Memory (BiLSTM) layer with a vertical direction of computation.\n",
    "It takes two arguments: input_size and hidden_size, which determine the input and hidden state dimensions of the LSTM.\n",
    "Inside the __init__ method, it initializes an nn.LSTM module with the specified input and hidden sizes. It's set to be bidirectional (bidirectional=True) and accepts input data in batches (batch_first=True).\n",
    "In the forward method, it applies the BiLSTM to the input tensor x and returns the output.\n",
    "BiLSTMHorizontal:\n",
    "This is another module similar to BiLSTMVertical, but it implements a BiLSTM layer with a horizontal direction of computation.\n",
    "It has the same constructor and forward method structure as BiLSTMVertical.\n",
    "FullyConnectedLayer:\n",
    "This module represents a fully connected (linear) layer in a neural network.\n",
    "It takes two arguments: input_size and output_size, determining the input and output dimensions of the linear layer.\n",
    "In the __init__ method, it initializes an nn.Linear module with the specified input and output sizes.\n",
    "In the forward method, it applies the linear transformation to the input tensor x and returns the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTMVertical(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BiLSTMVertical, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)\n",
    "        return output\n",
    "\n",
    "class BiLSTMHorizontal(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BiLSTMHorizontal, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)\n",
    "        return output\n",
    "\n",
    "class FullyConnectedLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FullyConnectedLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequencer2D\n",
    "Is a sequencer in which the input data (images) are splitted into vertical adn horiziontal component, then vertical data will be passed as input to BilstmVertical ,while horizontal_data will be passed to BiLSTMHorizontal, to do so the input data are first processed in such a way we'll have a tensor with vertical data of each image and another tensor with horizontal data of each image. Oncee we get the outputs from both BiLSTMvertical and BiLSTMHorizontal, we 'll concatenate them and pass as input the result to Fully Connected Layer . At the end of the sequencer the results will be a list of tensor ( each tensor has inside data belongs to each image processed above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Sequencer2D(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, mlp_input_size, mlp_output_size):\n",
    "        super(Sequencer2D, self).__init__()\n",
    "\n",
    "        # Create instances of the defined models\n",
    "        vertical_bilstm = BiLSTMVertical(input_size, hidden_size)\n",
    "        horizontal_bilstm = BiLSTMHorizontal(input_size, hidden_size)\n",
    "        fully_connected = FullyConnectedLayer( mlp_input_size,mlp_output_size)  \n",
    "\n",
    "        # Move models to GPU if available\n",
    "        #vertical_bilstm.to(device)\n",
    "        #horizontal_bilstm.to(device)\n",
    "        #fully_connected.to(device)\n",
    "\n",
    "        self.vertical_bilstm = vertical_bilstm\n",
    "        self.horizontal_bilstm = horizontal_bilstm\n",
    "        self.fully_connected = fully_connected\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, list):\n",
    "            x = torch.stack(x)\n",
    "        if len(x.shape)==4:\n",
    "            batch_size,channel, height, width = x.size()\n",
    "            x=x.squeeze(1)\n",
    "        elif len(x.shape) == 3 :\n",
    "            batch_size, height, width = x.size()\n",
    "        output=[]\n",
    "        for index in range(batch_size):\n",
    "            x_vertical = x[index][:, :height//2]  #split the image vertically\n",
    "            x_horizontal = x[index][ :,width//2:]   #split image horizontally\n",
    "\n",
    "            vertical_data = x_vertical.permute( 1,0) #permutation to fit bilstm\n",
    "            horizontal_data = x_horizontal.permute( 1,0)#same here\n",
    "            \n",
    "            \n",
    "            \n",
    "            H_ver = self.vertical_bilstm(vertical_data)\n",
    "            H_hor = self.horizontal_bilstm(horizontal_data)\n",
    "            \n",
    "            \n",
    "            H_ver = torch.tensor(H_ver, dtype=torch.float32).permute(1,0) #permute to fit torch.cat\n",
    "            H_hor = torch.tensor(H_hor, dtype=torch.float32).permute(1,0)\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            H_concat = torch.cat((H_ver, H_hor), dim=1)#concatenaiton of the horizontal and vertical bilstm outputs\n",
    "            H_concat = H_concat.unsqueeze(0) #set dimension to fit fully conncted layer\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            #fully connected layer\n",
    "            \n",
    "            \n",
    "            output_pieces = self.fully_connected(H_concat)\n",
    "            \n",
    "            output_pieces=torch.tensor(output_pieces, dtype=torch.float32)\n",
    "            output.append(output_pieces) #add output to lists of each image data processed here\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        \n",
    "        self.num_patches = (image_size // patch_size) ** 2 #compute the number of patches in such a way they won't overlap\n",
    "\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size) #convolutional layer takes the input image, divides it into patches, and embeds each patch into a lower-dimensional representation.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x should have the shape (B, C, H, W) if needed: B, C, H, W = x.shape\n",
    "     \n",
    "        x = self.projection(x).flatten(2).transpose(1, 2)  # (B reshape the input to split it into patches\n",
    "\n",
    "        return x\n",
    "    def output_dimension(self):\n",
    "        return self.embed_dim * self.num_patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PW Linear Layer compute  pointwise linear transformations on a list of input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PWLinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(PWLinearLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, input_list):\n",
    "        output_list = []\n",
    "        for input_tensor in input_list:\n",
    "            output_tensor = self.linear(input_tensor)\n",
    "            output_list.append(output_tensor)\n",
    "        stacked_output = torch.stack(output_list, dim=0)\n",
    "        return stacked_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch merging, take the images splitted in patches (for examples by patch embedding ) and merge it together, the number of patches merged is based on the output channlee and  the  scale factor number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor):\n",
    "        super(PatchMerging, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, patch_list):\n",
    "        \n",
    "        x = torch.cat(patch_list, dim=1).permute(1,2,0) # compination of the list of patches into a single tensor + peermutation to fit convolutional layer\n",
    "        \n",
    "        \n",
    "        x = self.conv(x) # Apply to the input x, a 1x1 convolution to merge the patches together\n",
    "        \n",
    "        \n",
    "        x = nn.functional.interpolate(x, scale_factor=self.scale_factor, mode='nearest') #resize the feature map using a nearest-neighbor upsampling\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAveragePooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAveragePooling, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.mean(x, dim=(-2, -1))# perform global average pooling along spatial dimensions using the mean\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequencer2Dmodel-> Is an architecture that resambles vision transformer except for the fact that Sequencer2D model is using BiLSTM2D (splitting image in horizzontal and vertical data) instead of self attention. It started with patch embedding layeer so the images will be divided in 8x8 patches, than ,after a normalization, there is the first sequence block of 4 Sequencer , the output of this block converge in PatchMerging, where the patches are resembled together ,then there is a second sequencer block made of 3 sequencer2d, followed by pointwise linear layer,then other sequencer block with 14 sequencers2D, then other point wise linear layer and  the final sequencerBlock with 3 sequencer2D layer and at the end there are linears+ global avarage pooling layers in order to make the output dimension as expected-\n",
    "\n",
    "The parameters are the one suggested by the paper\n",
    "This sequencer is Medium size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Sequencer2DModel(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels):\n",
    "        super(Sequencer2DModel, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "\n",
    "       \n",
    "        self.patch_embedding_1 = PatchEmbedding(16, 8, in_channels, 128)#  patch embedding with an 8x8 kernel size for each patch\n",
    "        self.ln_1 = nn.LayerNorm(128)\n",
    "\n",
    "        \n",
    "        self.sequencer_block_1 =  nn.Sequential(\n",
    "            Sequencer2D(16, 48, 72, 96),\n",
    "            Sequencer2D(96, 96, 96, 192),\n",
    "            Sequencer2D(192, 192, 192,384),\n",
    "            Sequencer2D(384, 192, 384,384)\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        self.patch_merging=PatchMerging(49152,128,2)\n",
    "\n",
    "      \n",
    "        self.sequencer_block_2 =  nn.Sequential(\n",
    "            Sequencer2D(384, 192, 3,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384)\n",
    "        )\n",
    "\n",
    "       \n",
    "        self.pw_linear_1 = PWLinearLayer( 2,384)\n",
    "        \n",
    "        \n",
    "        self.sequencer_block_3 =  nn.Sequential(\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384)\n",
    "            \n",
    "           \n",
    "        )\n",
    "\n",
    "        self.pw_linear_2 = PWLinearLayer(384, 384)\n",
    "\n",
    "       \n",
    "        self.sequencer_block_4 =  nn.Sequential(\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384)\n",
    "        )\n",
    "        self.pw_linear_3 = PWLinearLayer(384, 384)\n",
    "\n",
    "\n",
    "       \n",
    "        self.ln_2 = nn.LayerNorm(384)\n",
    "\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 384))\n",
    "\n",
    "        \n",
    "        self.fc = nn.Linear(384, num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "       \n",
    "        x = x.to(torch.float32).permute(0,3,1,2)  #convert input tensor to float32 + compute permutation\n",
    "        x = self.patch_embedding_1(x)\n",
    "        x = self.ln_1(x)\n",
    "\n",
    "       \n",
    "        x = self.sequencer_block_1(x)\n",
    "\n",
    "       \n",
    "        x = self.patch_merging(x)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        x = self.pw_linear_1(x)\n",
    "        \n",
    "       \n",
    "        x = self.pw_linear_3(x)\n",
    "\n",
    "       \n",
    "        x = self.ln_2(x)\n",
    "       \n",
    "        x = self.global_avg_pool(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "     \n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CustomCifar2, this custom function process imace into the dataset in order to take only the first two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomCIFAR2(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
    "        super(CustomCIFAR2, self).__init__()\n",
    "        self.cifar10 = datasets.CIFAR10(root, train=train, transform=transform, target_transform=target_transform, download=download)\n",
    "        \n",
    "        \n",
    "        self.keep_classes = [0, 1]  \n",
    "        self.data, self.targets = self.filter_classes()\n",
    "\n",
    "    def filter_classes(self):\n",
    "        mask = np.isin(self.cifar10.targets, self.keep_classes)\n",
    "        data = [self.cifar10.data[i] for i, include in enumerate(mask) if include]\n",
    "        targets = [self.cifar10.targets[i] for i, include in enumerate(mask) if include]\n",
    "        return data, targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model, with trnsform.Compose function that helps me with preprocessing and data augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "image_size = 32\n",
    "patch_size = 8\n",
    "model = Sequencer2DModel(num_classes=2, in_channels=3)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),   # Randomly flip the image horizontally\n",
    "    transforms.RandomRotation(15),      # Randomly rotate the image by up to 15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8, hue=0.8),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4),# Adjust brightness, contrast, saturation, and hue\n",
    "    transforms.RandomResizedCrop(16),\n",
    "    transforms.RandomResizedCrop(4),# Randomly crop and resize the image to 224x224\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.RandomAffine(degrees=4, translate=(0.4, 0.1)),# Randomly translate the image\n",
    "    transforms.ToTensor(),              # Convert the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#train_dataset = torchvision.datasets.CIFAR10(root='/Users/stellafazioli/Downloads/cifar-10-batches-py', train=True, transform=transform)\n",
    "custom_dataset = CustomCIFAR2(root='/Users/stellafazioli/Downloads/cifar-10-batches-py', train=True, transform=transform, download=True)\n",
    "data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequencer2D\n",
    "Is a sequencer in which the input data (images) are splitted into vertical adn horiziontal component, then vertical data will be passed as input to BilstmVertical ,while horizontal_data will be passed to BiLSTMHorizontal, to do so the input data are first processed in such a way we'll have a tensor with vertical data of each image and another tensor with horizontal data of each image. Oncee we get the outputs from both BiLSTMvertical and BiLSTMHorizontal, we 'll concatenate them and pass as input the result to Fully Connected Layer . At the end of the sequencer the results will be a list of tensor ( each tensor has inside data belongs to each image processed above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# beginning of training phase\n",
    "for epoch in range(num_epochs):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    running_loss = 0.0  #  loss for this epoch (but first 200 batch)\n",
    "\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        if i==78:\n",
    "            continue\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        outputs=outputs.view(128,2)\n",
    "        \n",
    "       \n",
    "        predicted_probabilities = torch.sigmoid(outputs)\n",
    "       \n",
    "        loss = criterion(predicted_probabilities[:,0], labels.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        predicted_probabilities = outputs.argmax(dim=1)\n",
    "        correct = (predicted_probabilities == labels).sum().item()\n",
    "        total_correct += correct\n",
    "        total_samples += labels.size(0)\n",
    "        print(predicted_probabilities.shape)\n",
    "        batch_accuracy = (correct / labels.size(0)) * 100.0\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{i}/{len(data_loader)}] Loss: {loss.item():.4f} Accuracy: {batch_accuracy:.2f}%\")\n",
    "        if i % 200 == 199:  # Print every 200 mini-batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{i}/{len(data_loader)}] Loss: {loss.item():.4f} Accuracy: {batch_accuracy:.2f}%\")\n",
    "            print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    epoch_accuracy = (total_correct / total_samples) * 100.0\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code used to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "model_pkl_file = \"sequencer_model384M_binary.pkl\"\n",
    "\n",
    "with open(model_pkl_file, 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for evaluation of the model over the dataset, here it is charged the test dataset as It was doing eiht the train dataset. Then the data are iterating and processed into the model to see if the prediciton match the actual labels, the code is way similar to the one used in the training phase except for the part of optimizer/backward ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-3fb609d5316e>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  H_ver = torch.tensor(H_ver, dtype=torch.float32).permute(1,0)\n",
      "<ipython-input-2-3fb609d5316e>:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  H_hor = torch.tensor(H_hor, dtype=torch.float32).permute(1,0)\n",
      "<ipython-input-2-3fb609d5316e>:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output_pieces=torch.tensor(output_pieces, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: [1/79] Test Accuracy: 56.25%\n",
      "2\n",
      "batch: [2/79] Test Accuracy: 52.34%\n",
      "3\n",
      "batch: [3/79] Test Accuracy: 49.74%\n",
      "4\n",
      "batch: [4/79] Test Accuracy: 50.98%\n",
      "5\n",
      "batch: [5/79] Test Accuracy: 50.78%\n",
      "6\n",
      "batch: [6/79] Test Accuracy: 50.00%\n",
      "7\n",
      "batch: [7/79] Test Accuracy: 50.11%\n",
      "8\n",
      "batch: [8/79] Test Accuracy: 49.22%\n",
      "9\n",
      "batch: [9/79] Test Accuracy: 49.74%\n",
      "10\n",
      "batch: [10/79] Test Accuracy: 50.31%\n",
      "11\n",
      "batch: [11/79] Test Accuracy: 50.36%\n",
      "12\n",
      "batch: [12/79] Test Accuracy: 50.26%\n",
      "13\n",
      "batch: [13/79] Test Accuracy: 49.94%\n",
      "14\n",
      "batch: [14/79] Test Accuracy: 49.61%\n",
      "15\n",
      "batch: [15/79] Test Accuracy: 49.84%\n",
      "16\n",
      "Test Accuracy: nan%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stellafazioli/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Users/stellafazioli/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.RandomRotation(15),     \n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    \n",
    "    transforms.RandomResizedCrop(16),\n",
    "    transforms.RandomResizedCrop(4),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.RandomAffine(degrees=4, translate=(0.4, 0.1)),\n",
    "    transforms.ToTensor(),            \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = CustomCIFAR2(\n",
    "    root='/Users/stellafazioli/Downloads/cifar-10-batches-py', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "model = Sequencer2DModel(num_classes=5, in_channels=3)\n",
    "\n",
    "# Load the pretrained model if you have saved it\n",
    "# model.load_state_dict(torch.load('your_model_weights.pth'))\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")#Gpu if avaliable\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "c=0\n",
    "\n",
    "total_correct=0\n",
    "total_samples=0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        \n",
    "        c+=1\n",
    "        print(c)\n",
    "        if c==16:\n",
    "            continue\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "       \n",
    "        \n",
    "        outputs = model(inputs)\n",
    "\n",
    "        \n",
    "        \n",
    "        predicted_labels = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        predicted_labels = predicted_labels.argmax(dim=1)\n",
    "        predicted_labels= predicted_labels.argmax(dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        correct = (predicted_labels == labels).sum().item()\n",
    "        \n",
    "        total_correct += correct\n",
    "        total_samples += len(labels)\n",
    "        \n",
    "        accuracy = (total_correct / total_samples) * 100.0  \n",
    "        print(f\"batch: [{c}/{len(data_loader)}]\",f\"Test Accuracy: {accuracy :.2f}%\")\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
